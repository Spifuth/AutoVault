#!/usr/bin/env bash
#===============================================================================
#
#  AUTOVAULT - run-tests.sh
#
#===============================================================================
#
#  DESCRIPTION:    Comprehensive test suite for AutoVault.
#                  Runs all unit tests, integration tests, and edge case
#                  tests with fancy terminal animations (when available).
#
#  TEST CATEGORIES:
#                  - Unit Tests        (requirements, syntax, JSON validity)
#                  - Integration Tests (structure creation, templates, validation)
#                  - Edge Cases        (empty lists, special characters, limits)
#                  - Idempotence Tests (running commands twice)
#                  - Invalid Config    (malformed JSON, wrong types)
#                  - Dry-Run Tests     (no modifications made)
#                  - Backup Tests      (create, list, restore)
#                  - Permission Tests  (read-only directories)
#
#  FEATURES:       - Animated spinners and progress bars (interactive mode)
#                  - CI mode detection (disables animations)
#                  - Colored output with pass/fail indicators
#                  - Final summary with success rate
#
#  USAGE:          ./tests/run-tests.sh
#
#  EXIT CODES:     0 - All tests passed
#                  1 - One or more tests failed
#
#  CI SUPPORT:     Automatically detects GitHub Actions and other CI
#                  environments, switches to simple text output.
#
#===============================================================================

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"

# Detect CI environment (no TTY, no TERM, or CI variable set)
CI_MODE=false
if [[ -n "${CI:-}" ]] || [[ -n "${GITHUB_ACTIONS:-}" ]] || [[ ! -t 1 ]] || [[ -z "${TERM:-}" ]]; then
    CI_MODE=true
    TERM="${TERM:-dumb}"
    export TERM
fi

# Colors (disabled in CI mode)
if [[ "$CI_MODE" == "true" ]]; then
    RED=''
    GREEN=''
    YELLOW=''
    BLUE=''
    CYAN=''
    MAGENTA=''
    WHITE=''
    BOLD=''
    DIM=''
    NC=''
else
    RED='\033[0;31m'
    GREEN='\033[0;32m'
    YELLOW='\033[0;33m'
    BLUE='\033[0;34m'
    CYAN='\033[0;36m'
    MAGENTA='\033[0;35m'
    WHITE='\033[1;37m'
    BOLD='\033[1m'
    DIM='\033[2m'
    NC='\033[0m' # No Color
fi

# Counters
TESTS_PASSED=0
TESTS_FAILED=0
TESTS_SKIPPED=0
CURRENT_TEST=0
TOTAL_TESTS=61

# Animation frames
SPINNER_FRAMES=("â ‹" "â ™" "â ¹" "â ¸" "â ¼" "â ´" "â ¦" "â §" "â ‡" "â ")
PROGRESS_CHARS=("â–‘" "â–’" "â–“" "â–ˆ")

#######################################
# Animation utilities
#######################################

# Hide/show cursor (no-op in CI mode)
hide_cursor() { 
    [[ "$CI_MODE" == "true" ]] && return
    echo -ne "\033[?25l"
}
show_cursor() { 
    [[ "$CI_MODE" == "true" ]] && return
    echo -ne "\033[?25h"
}

# Trap to restore cursor on exit
trap 'show_cursor' EXIT

# Spinner animation while running a command
spin() {
    local pid=$1
    local delay=0.1
    local frame=0
    
    while kill -0 "$pid" 2>/dev/null; do
        if [[ "$CI_MODE" == "false" ]]; then
            echo -ne "\r  ${CYAN}${SPINNER_FRAMES[$frame]}${NC} "
            frame=$(( (frame + 1) % ${#SPINNER_FRAMES[@]} ))
        fi
        sleep $delay
    done
    [[ "$CI_MODE" == "false" ]] && echo -ne "\r"
}

# Progress bar
draw_progress_bar() {
    [[ "$CI_MODE" == "true" ]] && return
    local current=$1
    local total=$2
    local width=30
    local percentage=$((current * 100 / total))
    local filled=$((current * width / total))
    local empty=$((width - filled))
    
    echo -ne "\r  ${DIM}[${NC}"
    for ((i=0; i<filled; i++)); do
        echo -ne "${GREEN}â–ˆ${NC}"
    done
    for ((i=0; i<empty; i++)); do
        echo -ne "${DIM}â–‘${NC}"
    done
    echo -ne "${DIM}]${NC} ${WHITE}${percentage}%${NC} (${current}/${total})"
}

# Animated banner
show_banner() {
    if [[ "$CI_MODE" == "true" ]]; then
        echo ""
        echo "========================================"
        echo "       AUTOVAULT TEST SUITE"
        echo "========================================"
        echo ""
        return
    fi
    clear
    echo ""
    echo -e "${CYAN}"
    cat << 'EOF'
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                               â•‘
    â•‘       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â•‘
    â•‘      â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•    â•‘
    â•‘      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—      â•‘
    â•‘      â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â•      â•‘
    â•‘      â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•   â–ˆâ–ˆâ•‘   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â•‘
    â•‘      â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â•    â•šâ•â•    â•šâ•â•â•â•â•â•    â•šâ•â•   â•šâ•â•â•â•â•â•â•    â•‘
    â•‘                                                               â•‘
    â•‘                    ğŸ§ª TEST SUITE ğŸ§ª                           â•‘
    â•‘                                                               â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EOF
    echo -e "${NC}"
    [[ "$CI_MODE" == "false" ]] && sleep 0.5
}

# Typing effect
type_text() {
    local text="$1"
    if [[ "$CI_MODE" == "true" ]]; then
        echo "$text"
        return
    fi
    local delay="${2:-0.03}"
    for ((i=0; i<${#text}; i++)); do
        echo -n "${text:$i:1}"
        sleep "$delay"
    done
    echo ""
}

# Category header with animation
show_category() {
    local name="$1"
    local icon="$2"
    echo ""
    if [[ "$CI_MODE" == "true" ]]; then
        echo "--- ${icon} ${name} ---"
    else
        echo -e "  ${MAGENTA}â”â”â”${NC} ${icon} ${BOLD}${WHITE}${name}${NC}"
        echo -e "  ${MAGENTA}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
        sleep 0.2
    fi
}

# Result animation
show_result() {
    local status="$1"
    local name="$2"
    
    case "$status" in
        pass)
            echo -e "    ${GREEN}âœ“${NC} ${name}"
            ;;
        fail)
            echo -e "    ${RED}âœ—${NC} ${name} ${RED}â† FAILED${NC}"
            ;;
        skip)
            echo -e "    ${YELLOW}â—‹${NC} ${name} ${DIM}(skipped)${NC}"
            ;;
    esac
}

#######################################
# Test utilities
#######################################

log_test() {
    echo -ne "    ${CYAN}â—‰${NC} ${DIM}$*${NC}"
}

log_pass() {
    echo -e "\r    ${GREEN}âœ“${NC} $*"
    ((TESTS_PASSED++))
    ((CURRENT_TEST++))
}

log_fail() {
    echo -e "\r    ${RED}âœ—${NC} $* ${RED}â† FAILED${NC}"
    ((TESTS_FAILED++))
    ((CURRENT_TEST++))
}

log_skip() {
    echo -e "\r    ${YELLOW}â—‹${NC} $* ${DIM}(skipped)${NC}"
    ((TESTS_SKIPPED++))
    ((CURRENT_TEST++))
}

# Run a test function and capture result
run_test() {
    local test_name="$1"
    local test_func="$2"
    
    if [[ "$CI_MODE" == "true" ]]; then
        # CI mode: simple output without animations
        echo -n "    Testing: ${test_name}... "
        
        local result=0
        $test_func &>/dev/null || result=1
        
        if [[ $result -eq 0 ]]; then
            echo "PASSED"
            ((TESTS_PASSED++))
        else
            echo "FAILED"
            ((TESTS_FAILED++))
        fi
        ((CURRENT_TEST++))
        return 0
    fi
    
    # Interactive mode with animations
    echo -ne "    ${CYAN}â—‰${NC} ${DIM}${test_name}${NC} "
    
    # Run test in background for spinner
    local result=0
    $test_func &>/dev/null &
    local pid=$!
    
    # Show spinner while test runs
    local frame=0
    while kill -0 "$pid" 2>/dev/null; do
        echo -ne "\r    ${CYAN}${SPINNER_FRAMES[$frame]}${NC} ${DIM}${test_name}${NC} "
        frame=$(( (frame + 1) % ${#SPINNER_FRAMES[@]} ))
        sleep 0.08
    done
    
    wait "$pid" || result=1
    
    if [[ $result -eq 0 ]]; then
        echo -e "\r    ${GREEN}âœ“${NC} ${test_name}                    "
        ((TESTS_PASSED++))
    else
        echo -e "\r    ${RED}âœ—${NC} ${test_name} ${RED}â† FAILED${NC}        "
        ((TESTS_FAILED++))
    fi
    ((CURRENT_TEST++))
    
    return 0  # Don't fail the whole suite
}

#######################################
# Setup / Teardown
#######################################

TEST_VAULT=""

setup() {
    # Create temporary test vault
    TEST_VAULT=$(mktemp -d)
    export TEST_VAULT
    
    # Create test config
    mkdir -p "$PROJECT_ROOT/config"
    cat > "$PROJECT_ROOT/config/cust-run-config.test.json" <<EOF
{
  "VaultRoot": "$TEST_VAULT",
  "CustomerIdWidth": 3,
  "CustomerIds": [1, 2, 3],
  "Sections": ["FP", "RAISED"],
  "TemplateRelativeRoot": "_templates/Run",
  "EnableCleanup": true
}
EOF
    
    export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
}

teardown() {
    # Clean up test vault
    if [[ -n "$TEST_VAULT" && -d "$TEST_VAULT" ]]; then
        rm -rf "$TEST_VAULT"
    fi
    
    # Clean up test config
    rm -f "$PROJECT_ROOT/config/cust-run-config.test.json"
}

#######################################
# Unit Tests
#######################################

test_requirements_check() {
    # Test that jq and python3 are available
    command -v jq >/dev/null 2>&1 && \
    command -v python3 >/dev/null 2>&1
}

test_config_json_valid() {
    # Test that main config templates are valid JSON
    [[ -f "$PROJECT_ROOT/config/templates.json" ]] && \
    jq empty "$PROJECT_ROOT/config/templates.json" 2>/dev/null && \
    [[ -f "$PROJECT_ROOT/config/obsidian-settings.json" ]] && \
    jq empty "$PROJECT_ROOT/config/obsidian-settings.json" 2>/dev/null
}

test_scripts_executable() {
    # Test that main scripts are executable
    [[ -x "$PROJECT_ROOT/cust-run-config.sh" ]] && \
    [[ -x "$PROJECT_ROOT/bash/New-CustRunStructure.sh" ]] && \
    [[ -x "$PROJECT_ROOT/bash/Manage-Templates.sh" ]]
}

test_scripts_syntax() {
    # Test bash syntax of all scripts
    local errors=0
    
    for script in "$PROJECT_ROOT"/*.sh "$PROJECT_ROOT/bash"/*.sh; do
        if [[ -f "$script" ]]; then
            if ! bash -n "$script" 2>/dev/null; then
                echo "  Syntax error in: $script"
                ((errors++))
            fi
        fi
    done
    
    [[ $errors -eq 0 ]]
}

test_help_command() {
    # Test that help works
    "$PROJECT_ROOT/cust-run-config.sh" --help >/dev/null 2>&1
}

test_version_command() {
    # Test that --version works and shows version info
    local output
    output=$("$PROJECT_ROOT/cust-run-config.sh" --version 2>&1)
    
    # Should contain version number
    echo "$output" | grep -q "AutoVault" && \
    echo "$output" | grep -q "version" && \
    echo "$output" | grep -qE "[0-9]+\.[0-9]+\.[0-9]+"
}

test_templates_preview() {
    # Test templates preview command
    export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
    
    local output
    output=$("$PROJECT_ROOT/cust-run-config.sh" templates preview root 2>&1)
    
    # Should contain preview markers and placeholder info
    echo "$output" | grep -q "Template:" && \
    echo "$output" | grep -q "CUST-001"
}

test_templates_preview_with_custom_id() {
    # Test templates preview with custom customer ID
    export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
    
    local output
    output=$("$PROJECT_ROOT/cust-run-config.sh" templates preview root 42 2>&1)
    
    # Should show CUST-042 (with padding)
    echo "$output" | grep -q "CUST-042"
}

test_templates_list() {
    # Test templates list command
    export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
    
    local output
    output=$("$PROJECT_ROOT/cust-run-config.sh" templates list 2>&1)
    
    # Should list available templates
    echo "$output" | grep -q "root" && \
    echo "$output" | grep -qi "section"
}

test_completion_files_exist() {
    # Test that completion files exist and are valid
    [[ -f "$PROJECT_ROOT/completions/autovault.bash" ]] && \
    [[ -f "$PROJECT_ROOT/completions/_autovault" ]] && \
    bash -n "$PROJECT_ROOT/completions/autovault.bash" 2>/dev/null
}

test_config_wizard_functions() {
    # Test that config wizard functions are defined
    source "$PROJECT_ROOT/bash/lib/config.sh"
    
    # Check that prompt functions exist
    declare -f prompt_value >/dev/null && \
    declare -f prompt_path >/dev/null && \
    declare -f prompt_list >/dev/null
}

test_prompt_path_expands_tilde() {
    # Test that prompt_path expands ~ correctly
    source "$PROJECT_ROOT/bash/lib/config.sh"
    
    # Simulate input with echo
    local result
    result=$(echo "" | prompt_path "test" "~/testpath" 2>/dev/null)
    
    # Should expand ~ to $HOME
    [[ "$result" == "$HOME/testpath" ]]
}

test_diff_mode_structure() {
    # Test diff mode for structure (needs structure to exist first)
    export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
    
    # Structure was already created by previous tests
    local output
    output=$("$PROJECT_ROOT/cust-run-config.sh" --diff structure 2>&1)
    
    # Should contain diff-related output (DIFF or diff or Summary or Structure)
    echo "$output" | grep -qiE "diff|summary|structure"
}

test_diff_command() {
    # Test diff command standalone
    export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
    
    local output
    output=$("$PROJECT_ROOT/cust-run-config.sh" diff 2>&1)
    
    # Should show diff output
    echo "$output" | grep -qi "diff"
}

test_stats_command() {
    # Test statistics command
    export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
    
    local output
    output=$("$PROJECT_ROOT/cust-run-config.sh" stats 2>&1)
    
    # Should contain statistics sections
    echo "$output" | grep -qi "statistics\|overview\|health"
}

test_customer_export() {
    # Test customer export
    export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
    
    # First create structure
    "$PROJECT_ROOT/cust-run-config.sh" structure >/dev/null 2>&1
    
    # Export customer 1
    local export_file="$TEST_VAULT/cust-export-test.tar.gz"
    "$PROJECT_ROOT/cust-run-config.sh" customer export 1 "$export_file" >/dev/null 2>&1
    
    # Verify archive exists and is valid
    [[ -f "$export_file" ]] && \
    tar -tzf "$export_file" | grep -q "CUST-001"
}

test_customer_clone() {
    # Test customer clone (uses export/import internally)
    export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
    
    # Clone customer 1 to 99
    "$PROJECT_ROOT/cust-run-config.sh" customer clone 1 99 >/dev/null 2>&1
    
    # Verify clone was created
    [[ -d "$TEST_VAULT/Run/CUST-099" ]]
}

#######################################
# Integration Tests
#######################################

test_structure_creation() {
    # Test creating folder structure
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
        bash ./cust-run-config.sh structure >/dev/null 2>&1
    )
    
    # Verify structure
    [[ -d "$TEST_VAULT/Run" ]] && \
    [[ -d "$TEST_VAULT/Run/CUST-001" ]] && \
    [[ -d "$TEST_VAULT/Run/CUST-002" ]] && \
    [[ -d "$TEST_VAULT/Run/CUST-003" ]] && \
    [[ -d "$TEST_VAULT/Run/CUST-001/CUST-001-FP" ]] && \
    [[ -d "$TEST_VAULT/Run/CUST-001/CUST-001-RAISED" ]] && \
    [[ -f "$TEST_VAULT/Run-Hub.md" ]]
}

test_templates_sync() {
    # Test syncing templates
    mkdir -p "$TEST_VAULT/_templates/Run"
    
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
        bash ./cust-run-config.sh templates sync >/dev/null 2>&1
    )
    
    # Verify templates exist
    [[ -f "$TEST_VAULT/_templates/Run/CUST-Root-Index.md" ]] && \
    [[ -f "$TEST_VAULT/_templates/Run/CUST-Section-FP-Index.md" ]]
}

test_templates_apply() {
    # Test applying templates (requires structure first)
    # Note: Structure must already exist from test_structure_creation
    export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
    
    (
        cd "$PROJECT_ROOT"
        bash ./cust-run-config.sh templates apply >/dev/null 2>&1
    )
    
    # Verify templates applied
    local index_file="$TEST_VAULT/Run/CUST-001/CUST-001-Index.md"
    [[ -f "$index_file" ]] && \
    grep -q "CUST-001" "$index_file"
}

test_validation() {
    # Test config validation
    export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
    "$PROJECT_ROOT/cust-run-config.sh" validate >/dev/null 2>&1
}

test_status_command() {
    # Test status command
    export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
    "$PROJECT_ROOT/cust-run-config.sh" status >/dev/null 2>&1
}

test_dry_run() {
    # Test that dry-run doesn't modify anything
    local before_count
    before_count=$(find "$TEST_VAULT" -type f 2>/dev/null | wc -l)
    
    export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
    "$PROJECT_ROOT/cust-run-config.sh" --dry-run structure >/dev/null 2>&1
    
    local after_count
    after_count=$(find "$TEST_VAULT" -type f 2>/dev/null | wc -l)
    
    [[ "$before_count" -eq "$after_count" ]]
}

test_verify_structure() {
    # Test structure verification (after creation)
    export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
    "$PROJECT_ROOT/cust-run-config.sh" test >/dev/null 2>&1
}

#######################################
# Edge Case Tests
#######################################

test_customer_id_zero() {
    # Test with customer ID = 0
    local test_config
    test_config=$(mktemp)
    cat > "$test_config" <<EOF
{
  "VaultRoot": "$TEST_VAULT",
  "CustomerIdWidth": 3,
  "CustomerIds": [0],
  "Sections": ["FP"],
  "TemplateRelativeRoot": "_templates/Run",
  "EnableCleanup": true
}
EOF
    
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$test_config"
        bash ./cust-run-config.sh structure >/dev/null 2>&1
    )
    
    local result=$?
    rm -f "$test_config"
    
    # Should create CUST-000
    [[ -d "$TEST_VAULT/Run/CUST-000" ]]
}

test_customer_id_large() {
    # Test with large customer ID (9999)
    local test_config
    test_config=$(mktemp)
    cat > "$test_config" <<EOF
{
  "VaultRoot": "$TEST_VAULT",
  "CustomerIdWidth": 4,
  "CustomerIds": [9999],
  "Sections": ["FP"],
  "TemplateRelativeRoot": "_templates/Run",
  "EnableCleanup": true
}
EOF
    
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$test_config"
        bash ./cust-run-config.sh structure >/dev/null 2>&1
    )
    
    rm -f "$test_config"
    
    # Should create CUST-9999
    [[ -d "$TEST_VAULT/Run/CUST-9999" ]]
}

test_customer_id_negative() {
    # Test with negative customer ID (should fail gracefully or handle)
    local test_config
    test_config=$(mktemp)
    cat > "$test_config" <<EOF
{
  "VaultRoot": "$TEST_VAULT",
  "CustomerIdWidth": 3,
  "CustomerIds": [-1],
  "Sections": ["FP"],
  "TemplateRelativeRoot": "_templates/Run",
  "EnableCleanup": true
}
EOF
    
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$test_config"
        bash ./cust-run-config.sh structure >/dev/null 2>&1
    ) || true
    
    rm -f "$test_config"
    
    # Test passes if no crash (negative IDs may create weird folders but shouldn't crash)
    true
}

test_empty_customer_list() {
    # Test with empty customer list - should error gracefully
    local test_config
    test_config=$(mktemp)
    cat > "$test_config" <<EOF
{
  "VaultRoot": "$TEST_VAULT",
  "CustomerIdWidth": 3,
  "CustomerIds": [],
  "Sections": ["FP"],
  "TemplateRelativeRoot": "_templates/Run",
  "EnableCleanup": true
}
EOF
    
    local result=0
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$test_config"
        bash ./cust-run-config.sh structure >/dev/null 2>&1
    ) || result=1
    
    rm -f "$test_config"
    
    # Script should fail with error when no customers defined
    [[ "$result" -eq 1 ]]
}

test_empty_sections_list() {
    # Test with empty sections list
    local test_config
    test_config=$(mktemp)
    local test_vault
    test_vault=$(mktemp -d)
    
    cat > "$test_config" <<EOF
{
  "VaultRoot": "$test_vault",
  "CustomerIdWidth": 3,
  "CustomerIds": [99],
  "Sections": [],
  "TemplateRelativeRoot": "_templates/Run",
  "EnableCleanup": true
}
EOF
    
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$test_config"
        bash ./cust-run-config.sh structure >/dev/null 2>&1
    )
    
    local result=0
    # Should create CUST folder
    [[ -d "$test_vault/Run/CUST-099" ]] || result=1
    
    rm -f "$test_config"
    rm -rf "$test_vault"
    
    [[ "$result" -eq 0 ]]
}

#######################################
# Idempotence Tests
#######################################

test_structure_idempotence() {
    # Running structure twice should not create duplicates or errors
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
        bash ./cust-run-config.sh structure >/dev/null 2>&1
    )
    
    local count_before
    count_before=$(find "$TEST_VAULT/Run" -type f | wc -l)
    
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
        bash ./cust-run-config.sh structure >/dev/null 2>&1
    )
    
    local count_after
    count_after=$(find "$TEST_VAULT/Run" -type f | wc -l)
    
    # File count should be the same
    [[ "$count_before" -eq "$count_after" ]]
}

test_templates_apply_idempotence() {
    # Applying templates twice should be safe and produce valid files
    # Note: We can't compare checksums because templates contain timestamps (NOW_UTC, NOW_LOCAL)
    
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
        bash ./cust-run-config.sh templates apply >/dev/null 2>&1
    )
    
    # Verify file exists and has content after first apply
    local index_file="$TEST_VAULT/Run/CUST-001/CUST-001-Index.md"
    [[ -f "$index_file" ]] || return 1
    local size_before
    size_before=$(wc -c < "$index_file")
    
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
        bash ./cust-run-config.sh templates apply >/dev/null 2>&1
    )
    
    # File should still exist with similar content (CUST code present)
    [[ -f "$index_file" ]] || return 1
    local size_after
    size_after=$(wc -c < "$index_file")
    
    # Size should be roughly the same (within 50 bytes for timestamp differences)
    local size_diff=$((size_after - size_before))
    [[ ${size_diff#-} -lt 50 ]] && grep -q "CUST-001" "$index_file"
}

#######################################
# Invalid Config Tests
#######################################

test_config_malformed_json() {
    # Test with malformed JSON
    local test_config
    test_config=$(mktemp)
    echo "{ invalid json }" > "$test_config"
    
    local result=0
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$test_config"
        bash ./cust-run-config.sh validate >/dev/null 2>&1
    ) || result=1
    
    rm -f "$test_config"
    
    # Should fail validation
    [[ $result -eq 1 ]]
}

test_config_missing_vaultroot() {
    # Test with missing VaultRoot - script uses defaults
    local test_config
    test_config=$(mktemp)
    cat > "$test_config" <<EOF
{
  "CustomerIdWidth": 3,
  "CustomerIds": [1],
  "Sections": ["FP"],
  "TemplateRelativeRoot": "_templates/Run",
  "EnableCleanup": true
}
EOF
    
    # Script should handle gracefully (use defaults or warn)
    # We just check it doesn't crash unexpectedly
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$test_config"
        bash ./cust-run-config.sh status >/dev/null 2>&1
    ) || true
    
    rm -f "$test_config"
    
    # Test passes as long as no crash
    true
}

test_config_wrong_types() {
    # Test with wrong types (string instead of array)
    local test_config
    test_config=$(mktemp)
    cat > "$test_config" <<EOF
{
  "VaultRoot": "$TEST_VAULT",
  "CustomerIdWidth": "three",
  "CustomerIds": "1,2,3",
  "Sections": "FP",
  "TemplateRelativeRoot": "_templates/Run",
  "EnableCleanup": "yes"
}
EOF
    
    # This might work or fail depending on implementation
    # Test just checks it doesn't crash
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$test_config"
        bash ./cust-run-config.sh validate >/dev/null 2>&1
    ) || true
    
    rm -f "$test_config"
    true
}

test_config_nonexistent() {
    # Test with non-existent config file
    # Script may use defaults or error - just check no crash
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="/nonexistent/path/config.json"
        bash ./cust-run-config.sh --help >/dev/null 2>&1
    )
    
    # Help should still work even with bad config path
    true
}

#######################################
# Special Path Tests
#######################################

test_vault_path_with_spaces() {
    # Test VaultRoot with spaces
    local test_vault_spaces
    test_vault_spaces=$(mktemp -d)
    local spaced_path="$test_vault_spaces/My Vault Path"
    mkdir -p "$spaced_path"
    
    local test_config
    test_config=$(mktemp)
    cat > "$test_config" <<EOF
{
  "VaultRoot": "$spaced_path",
  "CustomerIdWidth": 3,
  "CustomerIds": [1],
  "Sections": ["FP"],
  "TemplateRelativeRoot": "_templates/Run",
  "EnableCleanup": true
}
EOF
    
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$test_config"
        bash ./cust-run-config.sh structure >/dev/null 2>&1
    )
    
    local result=$?
    rm -f "$test_config"
    
    # Should create structure in path with spaces
    [[ -d "$spaced_path/Run/CUST-001" ]]
    local final_result=$?
    
    rm -rf "$test_vault_spaces"
    [[ $final_result -eq 0 ]]
}

test_section_name_special_chars() {
    # Test section name with special characters
    local test_config
    test_config=$(mktemp)
    cat > "$test_config" <<EOF
{
  "VaultRoot": "$TEST_VAULT",
  "CustomerIdWidth": 3,
  "CustomerIds": [50],
  "Sections": ["TEST-SECTION", "SECTION_2"],
  "TemplateRelativeRoot": "_templates/Run",
  "EnableCleanup": true
}
EOF
    
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$test_config"
        bash ./cust-run-config.sh structure >/dev/null 2>&1
    )
    
    rm -f "$test_config"
    
    # Should handle hyphens and underscores
    [[ -d "$TEST_VAULT/Run/CUST-050/CUST-050-TEST-SECTION" ]] && \
    [[ -d "$TEST_VAULT/Run/CUST-050/CUST-050-SECTION_2" ]]
}

#######################################
# Dry-Run Exhaustive Tests
#######################################

test_dry_run_structure_no_changes() {
    # Ensure dry-run creates nothing
    rm -rf "$TEST_VAULT/Run" 2>/dev/null || true
    
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
        bash ./cust-run-config.sh --dry-run structure >/dev/null 2>&1
    )
    
    # Run directory should NOT exist
    [[ ! -d "$TEST_VAULT/Run" ]]
}

test_dry_run_cleanup_no_deletion() {
    # Ensure dry-run cleanup doesn't delete anything
    mkdir -p "$TEST_VAULT/Run/CUST-001"
    echo "test" > "$TEST_VAULT/Run/CUST-001/test.md"
    
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
        bash ./cust-run-config.sh --dry-run cleanup >/dev/null 2>&1
    ) || true
    
    # File should still exist
    [[ -f "$TEST_VAULT/Run/CUST-001/test.md" ]]
}

test_dry_run_templates_no_write() {
    # Ensure dry-run templates doesn't write files
    mkdir -p "$TEST_VAULT/_templates/Run"
    
    local count_before
    count_before=$(find "$TEST_VAULT/_templates/Run" -type f | wc -l)
    
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$PROJECT_ROOT/config/cust-run-config.test.json"
        bash ./cust-run-config.sh --dry-run templates sync >/dev/null 2>&1
    )
    
    local count_after
    count_after=$(find "$TEST_VAULT/_templates/Run" -type f | wc -l)
    
    [[ "$count_before" -eq "$count_after" ]]
}

#######################################
# Permission Tests
#######################################

test_readonly_vault_dir() {
    # Test behavior when vault dir is read-only
    local readonly_vault
    readonly_vault=$(mktemp -d)
    chmod 555 "$readonly_vault"
    
    local test_config
    test_config=$(mktemp)
    cat > "$test_config" <<EOF
{
  "VaultRoot": "$readonly_vault",
  "CustomerIdWidth": 3,
  "CustomerIds": [1],
  "Sections": ["FP"],
  "TemplateRelativeRoot": "_templates/Run",
  "EnableCleanup": true
}
EOF
    
    local result=0
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$test_config"
        bash ./cust-run-config.sh structure >/dev/null 2>&1
    ) || result=1
    
    rm -f "$test_config"
    chmod 755 "$readonly_vault"
    rm -rf "$readonly_vault"
    
    # Should fail gracefully
    [[ $result -eq 1 ]]
}

#######################################
# Backup Tests
#######################################

test_backup_create() {
    # Test backup creation
    # Note: BACKUP_DIR defaults to $PROJECT_ROOT/backups, not vault/backups
    
    local test_config
    test_config=$(mktemp)
    cat > "$test_config" <<EOF
{
  "VaultRoot": "$TEST_VAULT",
  "CustomerIdWidth": 3,
  "CustomerIds": [1],
  "Sections": ["FP"],
  "TemplateRelativeRoot": "_templates/Run",
  "EnableCleanup": true
}
EOF
    
    local backup_count_before
    backup_count_before=$(find "$PROJECT_ROOT/backups" -name "*.json" -type f 2>/dev/null | wc -l)
    
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$test_config"
        bash ./cust-run-config.sh backup create test-backup >/dev/null 2>&1
    )
    
    local backup_count_after
    backup_count_after=$(find "$PROJECT_ROOT/backups" -name "*.json" -type f 2>/dev/null | wc -l)
    
    rm -f "$test_config"
    
    # Cleanup test backup
    find "$PROJECT_ROOT/backups" -name "*test-backup*" -type f -delete 2>/dev/null || true
    
    [[ $backup_count_after -gt $backup_count_before ]]
}

test_backup_list() {
    # Test backup listing
    # Create a fake backup in project backups dir
    mkdir -p "$PROJECT_ROOT/backups"
    local test_backup="$PROJECT_ROOT/backups/cust-run-config.2024-01-01_12-00-00.test.json"
    echo '{"test": 1}' > "$test_backup"
    
    local test_config
    test_config=$(mktemp)
    cat > "$test_config" <<EOF
{
  "VaultRoot": "$TEST_VAULT",
  "CustomerIdWidth": 3,
  "CustomerIds": [1],
  "Sections": ["FP"],
  "TemplateRelativeRoot": "_templates/Run",
  "EnableCleanup": true
}
EOF
    
    local output
    output=$(
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$test_config"
        bash ./cust-run-config.sh backup list 2>&1
    )
    
    rm -f "$test_config" "$test_backup"
    
    # Should list backups
    echo "$output" | grep -q "cust-run-config.2024-01-01"
}

test_backup_list_empty() {
    # Test backup listing when no backups exist
    # Move existing backups temporarily
    local temp_backup_dir
    temp_backup_dir=$(mktemp -d)
    if [[ -d "$PROJECT_ROOT/backups" ]]; then
        mv "$PROJECT_ROOT/backups"/*.json "$temp_backup_dir/" 2>/dev/null || true
    fi
    
    local test_config
    test_config=$(mktemp)
    cat > "$test_config" <<EOF
{
  "VaultRoot": "$TEST_VAULT",
  "CustomerIdWidth": 3,
  "CustomerIds": [1],
  "Sections": ["FP"],
  "TemplateRelativeRoot": "_templates/Run",
  "EnableCleanup": true
}
EOF
    
    local output
    output=$(
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$test_config"
        bash ./cust-run-config.sh backup list 2>&1
    )
    
    # Restore backups
    if [[ -d "$temp_backup_dir" ]]; then
        mv "$temp_backup_dir"/*.json "$PROJECT_ROOT/backups/" 2>/dev/null || true
        rm -rf "$temp_backup_dir"
    fi
    
    rm -f "$test_config"
    
    # Should indicate no backups
    echo "$output" | grep -qi "no backup"
}

test_backup_dry_run_no_create() {
    # Ensure dry-run doesn't create backups
    local backup_count_before
    backup_count_before=$(find "$PROJECT_ROOT/backups" -name "*.json" -type f 2>/dev/null | wc -l)
    
    local test_config
    test_config=$(mktemp)
    cat > "$test_config" <<EOF
{
  "VaultRoot": "$TEST_VAULT",
  "CustomerIdWidth": 3,
  "CustomerIds": [1],
  "Sections": ["FP"],
  "TemplateRelativeRoot": "_templates/Run",
  "EnableCleanup": true
}
EOF
    
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$test_config"
        bash ./cust-run-config.sh --dry-run backup create test >/dev/null 2>&1
    )
    
    local backup_count_after
    backup_count_after=$(find "$PROJECT_ROOT/backups" -name "*test*" -type f 2>/dev/null | wc -l)
    
    rm -f "$test_config"
    
    # Should not create new test backup files
    [[ $backup_count_after -eq $backup_count_before ]] || [[ $backup_count_after -eq 0 ]]
}

test_backup_cleanup_dry_run() {
    # Test backup cleanup dry-run
    mkdir -p "$PROJECT_ROOT/backups"
    
    # Create many fake backups
    for i in {01..15}; do
        echo "{\"test\": $i}" > "$PROJECT_ROOT/backups/cust-run-config.2024-01-${i}_12-00-00.cleanup-test.json"
    done
    
    local test_config
    test_config=$(mktemp)
    cat > "$test_config" <<EOF
{
  "VaultRoot": "$TEST_VAULT",
  "CustomerIdWidth": 3,
  "CustomerIds": [1],
  "Sections": ["FP"],
  "TemplateRelativeRoot": "_templates/Run",
  "EnableCleanup": true
}
EOF
    
    (
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$test_config"
        bash ./cust-run-config.sh --dry-run backup cleanup 5 >/dev/null 2>&1
    ) || true
    
    # Should still have all 15 backups (dry-run)
    local backup_count
    backup_count=$(find "$PROJECT_ROOT/backups" -name "*cleanup-test*" -type f 2>/dev/null | wc -l)
    
    # Cleanup test files
    find "$PROJECT_ROOT/backups" -name "*cleanup-test*" -type f -delete 2>/dev/null || true
    
    rm -f "$test_config"
    
    [[ $backup_count -eq 15 ]]
}

#######################################
# Hook Tests
#######################################

test_hooks_list() {
    # Test hooks list command
    local output
    output=$("$PROJECT_ROOT/cust-run-config.sh" hooks list 2>&1)
    
    # Should list available hooks
    echo "$output" | grep -q "pre-customer-remove" && \
    echo "$output" | grep -q "post-customer-remove" && \
    echo "$output" | grep -q "post-templates-apply" && \
    echo "$output" | grep -q "on-error"
}

test_hooks_init() {
    # Test hooks init command
    local test_hooks_dir="$TEST_VAULT/test-hooks-init"
    # Don't create directory - let init_hooks_dir create it
    rm -rf "$test_hooks_dir" 2>/dev/null || true
    
    (
        cd "$PROJECT_ROOT"
        bash ./cust-run-config.sh hooks init "$test_hooks_dir" >/dev/null 2>&1
    ) || true
    
    # Should create example files
    local result=true
    [[ -f "$test_hooks_dir/pre-customer-remove.sh.example" ]] || result=false
    [[ -f "$test_hooks_dir/post-templates-apply.sh.example" ]] || result=false
    [[ -f "$test_hooks_dir/on-error.sh.example" ]] || result=false
    [[ -f "$test_hooks_dir/README.md" ]] || result=false
    
    rm -rf "$test_hooks_dir"
    $result
}

test_hooks_pre_cancel() {
    # Test that a pre-hook can cancel an operation
    local test_hooks_dir="$TEST_VAULT/test-hooks-cancel"
    mkdir -p "$test_hooks_dir"
    
    # Create a pre-hook that returns non-zero
    cat > "$test_hooks_dir/pre-customer-remove.sh" << 'EOF'
#!/usr/bin/env bash
echo "Hook cancelling operation"
exit 1
EOF
    chmod +x "$test_hooks_dir/pre-customer-remove.sh"
    
    # Create test config with customer
    local test_config
    test_config=$(mktemp)
    cat > "$test_config" <<EOF
{
  "VaultRoot": "$TEST_VAULT",
  "CustomerIdWidth": 3,
  "CustomerIds": [42],
  "Sections": ["FP"],
  "TemplateRelativeRoot": "_templates/Run"
}
EOF

    # Try to remove customer - should fail due to hook
    local output
    output=$(
        cd "$PROJECT_ROOT"
        export CONFIG_JSON="$test_config"
        export AUTOVAULT_HOOKS_DIR="$test_hooks_dir"
        echo "y" | bash ./cust-run-config.sh customer remove 42 2>&1
    ) || true
    
    # Should contain cancellation message
    local result=false
    echo "$output" | grep -qi "cancel\|hook" && result=true
    
    rm -rf "$test_hooks_dir"
    rm -f "$test_config"
    $result
}

test_hooks_disabled() {
    # Test that hooks can be disabled
    local test_hooks_dir="$TEST_VAULT/test-hooks-disabled"
    mkdir -p "$test_hooks_dir"
    
    # Create a hook that would fail
    cat > "$test_hooks_dir/pre-customer-remove.sh" << 'EOF'
#!/usr/bin/env bash
exit 1
EOF
    chmod +x "$test_hooks_dir/pre-customer-remove.sh"
    
    # Test hook with hooks disabled - should not run hook
    (
        cd "$PROJECT_ROOT"
        export AUTOVAULT_HOOKS_ENABLED="false"
        export AUTOVAULT_HOOKS_DIR="$test_hooks_dir"
        # Source hooks.sh and test run_hook returns 0 (hook not run)
        source "$PROJECT_ROOT/bash/lib/hooks.sh"
        run_hook "pre-customer-remove" "test" 2>/dev/null
    )
    local exit_code=$?
    
    rm -rf "$test_hooks_dir"
    [[ $exit_code -eq 0 ]]
}

test_hooks_help() {
    # Test hooks help page
    local output
    output=$("$PROJECT_ROOT/cust-run-config.sh" hooks --help 2>&1)
    
    # Should contain hook descriptions
    echo "$output" | grep -qi "hook\|automation"
}

#######################################
# Remote Tests
#######################################

test_remote_init() {
    # Test remote init creates config file
    local test_config="$TEST_VAULT/remotes-test.json"
    rm -f "$test_config"
    
    (
        cd "$PROJECT_ROOT"
        export REMOTES_JSON="$test_config"
        bash ./cust-run-config.sh remote init >/dev/null 2>&1
    ) || true
    
    # Should create config file
    [[ -f "$test_config" ]] && \
    jq -e '.remotes' "$test_config" >/dev/null 2>&1
}

test_remote_add() {
    # Test adding a remote
    local test_config="$TEST_VAULT/remotes-add-test.json"
    
    # Create initial config
    echo '{"remotes": {}, "defaults": {"port": 22}}' > "$test_config"
    
    (
        cd "$PROJECT_ROOT"
        export REMOTES_JSON="$test_config"
        bash ./cust-run-config.sh remote add testserver user@example.com /path/to/vault >/dev/null 2>&1
    ) || true
    
    # Should have added the remote
    local host
    host=$(jq -r '.remotes.testserver.host' "$test_config" 2>/dev/null)
    
    rm -f "$test_config"
    [[ "$host" == "user@example.com" ]]
}

test_remote_remove() {
    # Test removing a remote
    local test_config="$TEST_VAULT/remotes-remove-test.json"
    
    # Create config with a remote
    cat > "$test_config" << 'EOF'
{
  "remotes": {
    "toremove": {
      "host": "user@server.com",
      "path": "/vault",
      "port": 22
    }
  }
}
EOF
    
    (
        cd "$PROJECT_ROOT"
        export REMOTES_JSON="$test_config"
        bash ./cust-run-config.sh remote remove toremove >/dev/null 2>&1
    ) || true
    
    # Should have removed the remote
    local result
    result=$(jq -r '.remotes | keys | length' "$test_config" 2>/dev/null)
    
    rm -f "$test_config"
    [[ "$result" == "0" ]]
}

test_remote_list() {
    # Test remote list command
    local test_config="$TEST_VAULT/remotes-list-test.json"
    
    cat > "$test_config" << 'EOF'
{
  "remotes": {
    "server1": {"host": "user@srv1.com", "path": "/v1", "port": 22}
  }
}
EOF
    
    local output
    output=$(
        cd "$PROJECT_ROOT"
        export REMOTES_JSON="$test_config"
        bash ./cust-run-config.sh remote list 2>&1
    ) || true
    
    rm -f "$test_config"
    echo "$output" | grep -q "server1"
}

test_remote_help() {
    # Test remote help page
    local output
    output=$("$PROJECT_ROOT/cust-run-config.sh" remote --help 2>&1)
    
    # Should contain remote descriptions
    echo "$output" | grep -qi "ssh\|rsync\|push\|pull"
}

#######################################
# TUI Tests
#######################################

test_tui_library_syntax() {
    # Test TUI library has valid syntax
    bash -n "$PROJECT_ROOT/bash/lib/tui.sh"
}

test_tui_flag_recognized() {
    # Test --tui flag is recognized (won't run in non-interactive)
    local output
    output=$("$PROJECT_ROOT/cust-run-config.sh" --tui 2>&1 </dev/null) || true
    
    # Should fail with "requires interactive terminal" or similar
    echo "$output" | grep -qi "terminal\|interactive\|tty" || [[ -z "$output" ]]
}

test_tui_help_documented() {
    # Test TUI is documented in help
    local output
    output=$("$PROJECT_ROOT/cust-run-config.sh" --help 2>&1)
    
    echo "$output" | grep -qi "tui\|interactive"
}

#######################################
# Subcommand Help Tests
#######################################

test_help_subcommands() {
    # Test all subcommand helps work
    local cmds=("templates" "customer" "section" "backup" "vault" "config" "structure" "hooks" "remote")
    local errors=0
    
    for cmd in "${cmds[@]}"; do
        if ! "$PROJECT_ROOT/cust-run-config.sh" "$cmd" --help >/dev/null 2>&1; then
            ((errors++))
        fi
    done
    
    [[ $errors -eq 0 ]]
}

test_no_color_flag() {
    # Test --no-color removes ANSI codes
    local output
    output=$("$PROJECT_ROOT/cust-run-config.sh" --no-color --help 2>&1)
    
    # Should not contain escape sequences
    ! echo "$output" | grep -q $'\033'
}

#######################################
# Final Summary Animation
#######################################

show_final_summary() {
    echo ""
    echo ""
    
    # Calculate percentage
    local total=$((TESTS_PASSED + TESTS_FAILED + TESTS_SKIPPED))
    local success_rate=0
    if [[ $total -gt 0 ]]; then
        success_rate=$((TESTS_PASSED * 100 / total))
    fi
    
    if [[ "$CI_MODE" == "true" ]]; then
        # Simple CI output
        echo "========================================"
        echo "           TEST RESULTS"
        echo "========================================"
        echo "  Passed:  $TESTS_PASSED"
        echo "  Failed:  $TESTS_FAILED"
        echo "  Skipped: $TESTS_SKIPPED"
        echo "  Success: ${success_rate}%"
        echo "========================================"
        echo ""
        if [[ $TESTS_FAILED -eq 0 ]]; then
            echo "All tests passed!"
        else
            echo "Some tests failed. Check the output above."
        fi
        echo ""
        return
    fi
    
    # Drum roll effect
    echo -ne "  ${DIM}Calculating results"
    for i in {1..5}; do
        echo -n "."
        sleep 0.2
    done
    echo -e "${NC}"
    sleep 0.3
    
    # Results box
    echo -e "  ${CYAN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
    echo -e "  ${CYAN}â•‘${NC}             ${BOLD}ğŸ“Š TEST RESULTS ğŸ“Š${NC}              ${CYAN}â•‘${NC}"
    echo -e "  ${CYAN}â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£${NC}"
    
    # Animated counters
    echo -ne "  ${CYAN}â•‘${NC}   ${GREEN}âœ“ Passed:${NC}  "
    for ((i=0; i<=TESTS_PASSED; i++)); do
        echo -ne "\r  ${CYAN}â•‘${NC}   ${GREEN}âœ“ Passed:${NC}  ${WHITE}${BOLD}$i${NC}                              "
        sleep 0.05
    done
    echo -e "  ${CYAN}â•‘${NC}"
    
    echo -ne "  ${CYAN}â•‘${NC}   ${RED}âœ— Failed:${NC}  "
    for ((i=0; i<=TESTS_FAILED; i++)); do
        echo -ne "\r  ${CYAN}â•‘${NC}   ${RED}âœ— Failed:${NC}  ${WHITE}${BOLD}$i${NC}                              "
        sleep 0.05
    done
    echo -e "  ${CYAN}â•‘${NC}"
    
    echo -ne "  ${CYAN}â•‘${NC}   ${YELLOW}â—‹ Skipped:${NC} "
    for ((i=0; i<=TESTS_SKIPPED; i++)); do
        echo -ne "\r  ${CYAN}â•‘${NC}   ${YELLOW}â—‹ Skipped:${NC} ${WHITE}${BOLD}$i${NC}                              "
        sleep 0.05
    done
    echo -e "  ${CYAN}â•‘${NC}"
    
    echo -e "  ${CYAN}â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£${NC}"
    
    # Success rate with visual bar
    local bar_width=25
    local filled=$((success_rate * bar_width / 100))
    local empty=$((bar_width - filled))
    
    echo -ne "  ${CYAN}â•‘${NC}   Success: "
    for ((i=0; i<filled; i++)); do
        if [[ $success_rate -ge 80 ]]; then
            echo -ne "${GREEN}â–ˆ${NC}"
        elif [[ $success_rate -ge 50 ]]; then
            echo -ne "${YELLOW}â–ˆ${NC}"
        else
            echo -ne "${RED}â–ˆ${NC}"
        fi
    done
    for ((i=0; i<empty; i++)); do
        echo -ne "${DIM}â–‘${NC}"
    done
    echo -e " ${WHITE}${BOLD}${success_rate}%${NC}   ${CYAN}â•‘${NC}"
    
    echo -e "  ${CYAN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo ""
    
    # Final status with celebration or consolation
    if [[ $TESTS_FAILED -eq 0 ]]; then
        # Success celebration
        echo ""
        echo -e "  ${GREEN}${BOLD}â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®${NC}"
        echo -e "  ${GREEN}${BOLD}â”‚    ğŸ‰ ALL TESTS PASSED! AMAZING! ğŸ‰         â”‚${NC}"
        echo -e "  ${GREEN}${BOLD}â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯${NC}"
        
        # Confetti animation
        local confetti=("ğŸŠ" "âœ¨" "ğŸŒŸ" "ğŸ’«" "â­" "ğŸˆ" "ğŸ" "ğŸ†")
        echo ""
        echo -n "  "
        for i in {1..20}; do
            echo -n "${confetti[$((RANDOM % ${#confetti[@]}))]} "
            sleep 0.05
        done
        echo ""
        echo ""
        
        # Victory message
        echo -e "  ${WHITE}${BOLD}Your code is rock solid! ğŸª¨${NC}"
        echo ""
    else
        # Failure message - but supportive!
        echo ""
        echo -e "  ${YELLOW}${BOLD}â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®${NC}"
        echo -e "  ${YELLOW}${BOLD}â”‚    âš ï¸  Some tests need attention âš ï¸         â”‚${NC}"
        echo -e "  ${YELLOW}${BOLD}â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯${NC}"
        echo ""
        echo -e "  ${WHITE}Don't worry! You've got this! ğŸ’ª${NC}"
        echo -e "  ${DIM}Check the failed tests above and try again.${NC}"
        echo ""
    fi
}

#######################################
# Main
#######################################

main() {
    hide_cursor
    
    # Show animated banner
    show_banner
    
    # Setup phase with animation  
    if [[ "$CI_MODE" == "true" ]]; then
        echo "Setting up test environment..."
        setup
        echo "Test environment ready!"
        echo "Test vault: $TEST_VAULT"
        echo ""
    else
        echo -ne "  ${CYAN}âš™${NC} Setting up test environment"
        
        # Show spinner for visual effect
        local frame=0
        for i in {1..10}; do
            echo -ne "\r  ${CYAN}${SPINNER_FRAMES[$frame]}${NC} Setting up test environment"
            frame=$(( (frame + 1) % ${#SPINNER_FRAMES[@]} ))
            sleep 0.08
        done
        
        # Actually run setup (needs to be in foreground for variable export)
        setup
        
        echo -e "\r  ${GREEN}âœ“${NC} Test environment ready!              "
        echo -e "  ${DIM}Test vault: $TEST_VAULT${NC}"
        echo ""
        sleep 0.3
    fi
    
    # Unit tests
    show_category "UNIT TESTS" "ğŸ”¬"
    run_test "Requirements available" test_requirements_check || true
    run_test "Config JSON files valid" test_config_json_valid || true
    run_test "Scripts are executable" test_scripts_executable || true
    run_test "Scripts have valid syntax" test_scripts_syntax || true
    run_test "Help command works" test_help_command || true
    run_test "Version command works" test_version_command || true
    run_test "Subcommand helps work" test_help_subcommands || true
    run_test "No-color flag works" test_no_color_flag || true
    run_test "Completion files exist" test_completion_files_exist || true
    run_test "Config wizard functions" test_config_wizard_functions || true
    run_test "Prompt path expands tilde" test_prompt_path_expands_tilde || true
    
    # Integration tests
    show_category "INTEGRATION TESTS" "ğŸ”—"
    run_test "Dry-run mode" test_dry_run || true
    run_test "Structure creation" test_structure_creation || true
    run_test "Templates sync" test_templates_sync || true
    run_test "Templates apply" test_templates_apply || true
    run_test "Templates preview" test_templates_preview || true
    run_test "Templates preview custom ID" test_templates_preview_with_custom_id || true
    run_test "Templates list" test_templates_list || true
    run_test "Diff mode structure" test_diff_mode_structure || true
    run_test "Diff command" test_diff_command || true
    run_test "Statistics command" test_stats_command || true
    run_test "Customer export" test_customer_export || true
    run_test "Customer clone" test_customer_clone || true
    run_test "Configuration validation" test_validation || true
    run_test "Status command" test_status_command || true
    run_test "Structure verification" test_verify_structure || true
    
    # Edge case tests
    show_category "EDGE CASE TESTS" "ğŸ”"
    run_test "Customer ID zero" test_customer_id_zero || true
    run_test "Customer ID large (9999)" test_customer_id_large || true
    run_test "Customer ID negative" test_customer_id_negative || true
    run_test "Empty customer list" test_empty_customer_list || true
    run_test "Empty sections list" test_empty_sections_list || true
    run_test "Vault path with spaces" test_vault_path_with_spaces || true
    run_test "Section with special chars" test_section_name_special_chars || true
    
    # Idempotence tests
    show_category "IDEMPOTENCE TESTS" "ğŸ”„"
    run_test "Structure idempotence" test_structure_idempotence || true
    run_test "Templates apply idempotence" test_templates_apply_idempotence || true
    
    # Invalid config tests
    show_category "INVALID CONFIG TESTS" "âš ï¸"
    run_test "Malformed JSON config" test_config_malformed_json || true
    run_test "Missing VaultRoot" test_config_missing_vaultroot || true
    run_test "Wrong types in config" test_config_wrong_types || true
    run_test "Non-existent config file" test_config_nonexistent || true
    
    # Dry-run exhaustive tests
    show_category "DRY-RUN TESTS" "ğŸŒµ"
    run_test "Dry-run structure no changes" test_dry_run_structure_no_changes || true
    run_test "Dry-run cleanup no deletion" test_dry_run_cleanup_no_deletion || true
    run_test "Dry-run templates no write" test_dry_run_templates_no_write || true
    
    # Backup tests
    show_category "BACKUP TESTS" "ğŸ’¾"
    run_test "Backup create" test_backup_create || true
    run_test "Backup list" test_backup_list || true
    run_test "Backup list empty" test_backup_list_empty || true
    run_test "Backup dry-run no create" test_backup_dry_run_no_create || true
    run_test "Backup cleanup dry-run" test_backup_cleanup_dry_run || true
    
    # Permission tests
    show_category "PERMISSION TESTS" "ğŸ”"
    run_test "Read-only vault directory" test_readonly_vault_dir || true
    
    # Hook tests
    show_category "HOOK TESTS" "ğŸª"
    run_test "Hooks list command" test_hooks_list || true
    run_test "Hooks init command" test_hooks_init || true
    run_test "Hooks pre-hook cancellation" test_hooks_pre_cancel || true
    run_test "Hooks disabled" test_hooks_disabled || true
    run_test "Hooks help page" test_hooks_help || true
    
    # Remote tests
    show_category "REMOTE TESTS" "ğŸŒ"
    run_test "Remote init command" test_remote_init || true
    run_test "Remote add command" test_remote_add || true
    run_test "Remote remove command" test_remote_remove || true
    run_test "Remote list command" test_remote_list || true
    run_test "Remote help page" test_remote_help || true
    
    # TUI tests
    show_category "TUI TESTS" "ğŸ–¥ï¸"
    run_test "TUI library syntax" test_tui_library_syntax || true
    run_test "TUI flag recognized" test_tui_flag_recognized || true
    run_test "TUI help documented" test_tui_help_documented || true
    
    # Teardown with animation
    echo ""
    if [[ "$CI_MODE" == "true" ]]; then
        echo "Cleaning up..."
        teardown
        echo "Cleanup complete!"
    else
        echo -ne "  ${CYAN}âš™${NC} Cleaning up"
        
        # Show spinner for visual effect
        local frame=0
        for i in {1..8}; do
            echo -ne "\r  ${CYAN}${SPINNER_FRAMES[$frame]}${NC} Cleaning up"
            frame=$(( (frame + 1) % ${#SPINNER_FRAMES[@]} ))
            sleep 0.06
        done
        
        teardown
        echo -e "\r  ${GREEN}âœ“${NC} Cleanup complete!              "
    fi
    
    # Show final summary
    show_final_summary
    
    show_cursor
    
    if [[ $TESTS_FAILED -gt 0 ]]; then
        exit 1
    fi
    exit 0
}

main "$@"
